{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1MOspw1L_uy"
      },
      "source": [
        "Este repositorio, es una brebe introducción para entender los primeros pasos que daremos como científicos de datos.\n",
        "\n",
        "De cierta forma, este repositorio, me ayudó a ir documentando cada paso que fui aprendiendo en el proceso de aprendizaje hasta lograr hacer scripts en forma automática. Podríamos decir que, repitiendo estos pasos, haciendo conciencia de cada uno de los códigos (pensar: qué debo o quiero hacer aquí, qué código se usa para ello), llegaremos a ese momento en el que naturalizaremos el proceso y nos volveremos ágiles o prácticos en nuestros futuros trabajos. Pero como todo, para naturalizar tantos códigos, dependeremos de la **práctica, práctica, y más práctica**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1sPSrDcPoVA"
      },
      "source": [
        "# **Documentación y Reportes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRD38qACOaGy"
      },
      "source": [
        "###### *Advertencia: Este Módulo será teórico no técnico y muuuy resumido porque no pretende ser un manual, más bien, un apunte de notas rápidas*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eod0X0_vQrla"
      },
      "source": [
        "Cuando creamos un proytecto o informe, normalmente los trabajaremos sobre un **IDE** (un programa \"Traductor\" con atajos agíles y terminales) del lenguaje de programación (python, R, SQL, Java, etc.)\n",
        "Entre los IDE más reconocidos tenemos: google colab, VisualStudio Code, Jupyter, entre otros. En estos programas escribiremos nuestros **Notebooks**: scripts, proyectos, informes, apuntes... un machete...\n",
        "\n",
        "Por más que nosotros tengamos naturalizados los códigos, debemos entender que no trabajamos solos y que otros podrán hacer aportes o trabajar con nuestros notebooks compartidos, por lo que se recomienda la buena costumbre de ir apuntando notas de cada paso que demos, tambíen nos servirá como recordatorio de lo que estamos haciendo en cada línea de código. Principalmente si recién empezamos.\n",
        "\n",
        "Para ello en la caja de código vamos a hacer uso de **\"#\" + *comentario***. Al usar este símbolo al principio de la línea de código, el programa no lo lee, lo ignora a la hora de ejecutar en la terminal, por lo que no afecta al resultado del código que estemos trabajando.\n",
        "\n",
        "¡Y Creanme! al usarlo, evitarán perder el tiempo intentando recordar por qué pusieron eso, para qué servía esa línea de código, qué me está mostrando esa línea de código o que un colega les haga estas consultas.\n",
        "\n",
        "Por otro lado, tenemos los **Markdown**: son muy útiles a la hora de documentar los proyectos, en cada paso que damos, así como lo uso ahora para redactar texto dentro del Notebook:\n",
        "\n",
        "* Poner Títulos\n",
        "* Crear Cajas de Texto de documentación: Explicar tus decisiones *¿por qué realizo éste paso de este módo?*\n",
        "* Marcar Hipótesis sobre las que estamos trabajando\n",
        "* Crear Conclusiones\n",
        "* etc.\n",
        "\n",
        "Como científicos de datos, es muy importante ser organizados en nuestros procesos, como sabemos:\n",
        "\n",
        "El primer elemento que tenemos son las **necesidades del cliente** : aquello de debemos estudiar, analizar e intentar resolver o encontrar.\n",
        "\n",
        "En segundo lugar, nos entregarán los **datos** con los que trabajaremos, los cuales necesitan un **Procesamiento**: limpieza, estructura, agruparlos para conseguir información.\n",
        "\n",
        "En el proceso de limpieza, nos encontremos con las **hpótesis** a validar, éstas no se deben ignorar, un buen profesional, las toma como una prioridad, porque las que tiene un cliente, son la necesidad de saber es consulta a ciegas de parte del cliente, porque sabe lo que necesita, pero muchas veces, no sabe el verdadero valor que tienen hasta no transformar ese conjunto de datos dispersos en ordenados, en información relevante.*\n",
        "\n",
        "A esta información la **analizaremos**, haremos un procesamiento para darle sentido a esta información (si la tiene).\n",
        "\n",
        "Llegado este punto en el que ya podemos **interpretar la información** es bueno crear **gráficas** que acompañen los resultados, creando una estética visual más comprensible y rápida de lo que estamos escrudiñando. Así, resolveremos nuestras primeras **conclusiones**. Digo las primeras, porque si bien uno comienza en base a una hipótesis, al comprobarla y/o resolverla, nos damos cuenta, siempre, que esto nos lleva a nuevas hipótesis, podemos alcanzar soluciones más eficientes, respuestas a incógnitas no previstas, ver una oportunidad escondida.\n",
        "\n",
        "Por ello, siempre hablamos de que **el análisis de datos es un proceso iterativo hasta conocer las respuestas originales**.\n",
        "\n",
        "Estas conclusiones las convertiremos en entregables, y quí debemos percatarnos de quién lo leerá, así que, al escribirlas, debemos pensar si va dirigido a un grupo de ingenieros (a quienes les importará conocer el proceso realizado, los tecnicismos) o a los dueños de la empresa (a quienes solo les importará ver Visualizaciones Gráficas, Resultados, Conclusiones para la Toma de Decisiones futuras)\n",
        "\n",
        "\n",
        "Ahora sí, los códigos que nos competen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj_yMCySVnms"
      },
      "source": [
        "# **Procesamiento de Datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "togD-hwzO_kf"
      },
      "source": [
        "### 1°. **Importación de Librerías y Archivos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETFB5xO6F9i3"
      },
      "source": [
        "Las librerías son paquetes de códigos extensos resumidos en pequeñas órdenes que nos facilitarán el trabajo. Cada librería contiene funciones y métodos específicos para ciertas tareas.\n",
        "\n",
        "**Dataframes: Pandas y Numpy.** Procesamiento y Análisis de Datos\n",
        "\n",
        "**Visualización de datos: Matplotlib y Seaborn.** Creación de gráficas\n",
        "\n",
        "Estas librerías son las más destacadas para el procesamiento de los datos. Los datos que vamos a trabajar pueden ser: .csv, .xlsx, .json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y88Fqv6UPXeI"
      },
      "outputs": [],
      "source": [
        "#importar +  librería + as + apodo. ej.:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#importar datos:\n",
        "# Le decimos a pandas que lea el tipo de archivo que se encuentra en tal lugar y tiene caracteres pertenecientes al español.\n",
        "# si el archivo no tiene caracteres especiales de nuestro dialecto, omitir este paso.\n",
        "#apodo archivos = pd.read_csv(\"ruta de archivo\", reconocer lectura española: ñ y acentos) ej.:\n",
        "poblacion = pd.read_csv(\"rutadearchivo\", encoding='latin1')\n",
        "# al decirle a pandas que lea un archivo, automáticamente lo estructura en un dataframe(conjunto de datos planos contenidos en una tabla con filas y columnas), normalmente estos archivos son datos contenidos en tuplas.\n",
        "\n",
        "#Fijar Ruta en la que trabajaremos. (con los inputs y outputs)\n",
        "os.chdir(\"Ruta de trabajo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YufppuFRMRJd"
      },
      "source": [
        "### 2°.**Visualización de Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JcJ0nK-RNSsb"
      },
      "outputs": [],
      "source": [
        "#Visualizar DataFrame. Ponemos el nombre asignado al archivo como en este caso:\n",
        "poblacion\n",
        "#Función head: ver primeras 5 filas\n",
        "poblacion.head()\n",
        "\n",
        "#Función head: ver primeras 15 filas\n",
        "poblacion.head(15)\n",
        "\n",
        "#Función \"tail\" ver las últimas 5 filas\n",
        "poblacion.tail()\n",
        "\n",
        "#Función shape: Verificamos cantidad de filas, cantidad de columnas\n",
        "poblacion.shape\n",
        "\n",
        "#Función dtypes: verificamos los tipos de variables del dataset\n",
        "poblacion.dtypes\n",
        "\n",
        "# La terminal nos devuelve: nombre de columna + tipo devariable:\n",
        "#provincia            object\n",
        "#anio                  int64\n",
        "#poblacion_total       int64\n",
        "#poblacion_varones     int64\n",
        "#poblacion_mujeres     int64\n",
        "#dtype: object\n",
        "\n",
        "#Función astypes: cambiamos los tipos de variables del dataset\n",
        "poblacion.astypes()\n",
        "\n",
        "#Función describe: calculamos las principales estadísticas descriptivas: (promedio, mediana, min. y max)\n",
        "poblacion.describe()\n",
        "\n",
        "# Perfilado Básico de los datos:\n",
        "print(df.info())\n",
        "print(\"\\n\\u001b[1mResumen estadístico:\\u001b[0m\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOselp5FP93t"
      },
      "source": [
        "### 3°.**Estructurar Datos del Dataframe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcdR01T3fj9r"
      },
      "source": [
        " Limpieza de datos: detección y corrección de errores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF3Jvr79UtnU"
      },
      "outputs": [],
      "source": [
        "# Funciones de eliminación & Missing:\n",
        "poblacion.isnull()              # Devuelve un DataFrame booleano del mismo tamaño con True donde hay valores nulos (NaN)\n",
        "poblacion.isnull().sum()        # Devuelve la cantidad de valores nulos por columna\n",
        "poblacion.fillna(0)             # Reemplaza todos los valores nulos por 0 (sin modificar el original, salvo que uses inplace=True)\n",
        "poblacion.drop()                # Elimina filas por defecto, o columnas si se especifica axis=1 (requiere argumentos como labels o axis)\n",
        "poblacion.dropna()              # Elimina las filas que contienen al menos un valor nulo\n",
        "\n",
        "# Duplicados:\n",
        "duplicados = poblacion.duplicated().sum()       # Devuelve la cantidad de valores duplicados por columna\n",
        "print(f\"Registros duplicados: {duplicados}\")    # imprime los valores duplicados por columna de la variable \"duplicados\"\n",
        "poblacion.duplicated(inplace=True)              # Devuelve una serie booleana indicando si una fila está duplicada (True si ya apareció una igual antes)\n",
        "poblacion.drop_duplicates()                     # Elimina las filas duplicadas, dejando solo la primera ocurrencia\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transformación y codificación de variables categóricas,\n",
        "**Si** hay variables categóricas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categoricas = df.select_dtypes(include='object').columns\n",
        "print(f\"Variables categóricas: {list(categoricas)}\")\n",
        "\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfwG9ChjfuR_"
      },
      "source": [
        "Filtros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npO1vq8lQYUr"
      },
      "outputs": [],
      "source": [
        "#Importamos nuevamente el arcvhivo pero sin las últimas 3 filas.\n",
        "#Función: \"skipfooter\" elimina la cantidad deseada de últimas filas.\n",
        "#Código: \"engine=\" para especificar el tipo de archivo csv que tengo.\n",
        "# alias = pd.read_csv(“datos_alias.csv”, skipfooter = 3, engine = ‘python’)\n",
        "poblacion = pd.read_csv(\"rutadearchivo\", encoding='latin1', skipfooter = 3, engine = 'python')\n",
        "\n",
        "\n",
        "#Filtrar observaciones/filas que correspondan a variables específicas.\n",
        "# Se crea sobre una copia, un segundo Data Frame.\n",
        "# &= y, además;  //=\n",
        "#.issin: te permite filtrar varios valores, al mismo tiempo, de la misma columna.\n",
        "#De mi dataset \"alias\", quiero columna x y me quiero quedar,\".issin\", con los valores \"Dato 1,2,3\"\n",
        "alias_f =  alias [(alias[\"Columna X\"].issin([ \"Dato 1\", \"Dato 2\", \"Dato 3\" ]))&\n",
        "#Además, de la \"Columna Z\" quiero solo los valores de \"Dato j,n\"\n",
        "                  (alias [\"Colunma Z\"] == \"Dato j, Dato n\")&\n",
        "#Y que de la \"Columna T\" me queden los valores de \"Dato b\"\n",
        "\t                (alias [\"Columna T\"] == \"Dato b\")]\n",
        "alias_f\n",
        "\n",
        "poblacion_f = poblacion\n",
        "# Filtrar por los años de interés\n",
        "poblacion_f = poblacion[(poblacion['anio'].isin([\"2015\", \"2020\", \"2025\", \"2030\", \"2035\", \"2040\"])) &\n",
        "# Eliminar las filas con 'Total País' en la columna 'provincia'\n",
        "             (poblacion ['provincia'] != 'Total País')]\n",
        "poblacion_f\n",
        "\n",
        "# Valores fuera de rango o inconsistentes (apply(), filtros). ej:\n",
        "poblacion.apply(lambda x: x.min())   # Aplica una función a cada columna (en este caso, devuelve el valor mínimo por columna)\n",
        "poblacion.apply(lambda x: x.max())   # Aplica una función a cada columna (en este caso, devuelve el valor máximo por columna)\n",
        "poblacion['edad'] = poblacion['edad'].apply(lambda x: None if x < 0 or x > 120 else x)    # Eliminar edades imposibles\n",
        "poblacion['pais'] = poblacion['pais'].str.strip().str.lower()    # Corregir espacios (.strip) y mayúsculas (lower pasa el texto a minúsculas)\n",
        "\n",
        "# Para visualizar en la terminal, cualquiera de estas líneas de código usaremos la función de imprimir el resultado:\n",
        "print(poblacion.isnull())                  # Imprime un DataFrame con True/False según haya valores nulos\n",
        "print(poblacion.isnull().sum())      # Imprime la cantidad de valores nulos por columna\n",
        "print(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5j9_CGQX9gi"
      },
      "source": [
        "### 4°.**Normalización, transformación y codificación**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03Z1MYdaYMtf"
      },
      "outputs": [],
      "source": [
        "#Normalización / escalado de variables numéricas. Uso de SKlearn\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = df_encoded.copy()\n",
        "df[['ingresos_normalizados']] = scaler.fit_transform(df[['ingresos']])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "#df_scaled[columnas_numericas] = scaler.fit_transform(df_scaled[columnas_numericas])\n",
        "df[['edad_estandarizada']] = scaler.fit_transform(df[['edad']])\n",
        "\n",
        "#Transformaciones estadísticas. Uso de Numpy\n",
        "import numpy as np\n",
        "\n",
        "df['log_ingreso'] = np.log1p(df['ingresos'])  # logaritmo para reducir asimetría\n",
        "df['ingresos_cuadrado'] = df['ingresos'] ** 2\n",
        "\n",
        "#Codificación de variables categóricas\n",
        "pd.get_dummies(df['genero'], prefix='genero')          # one-hot encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['genero_codificado'] = le.fit_transform(df['genero'])  # masculino=1, femenino=0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOXhont3Y6a5"
      },
      "source": [
        "### 5°. **Exploración y perfilado básico**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLFEJH_GZaCS"
      },
      "source": [
        "Las siguientes visualizaciones nos permiten por un lado entender los datos de una manera más gáfica, principalmente si tenemos datos que se disparan, serán más fáciles de detectar y evaluar si será mejor analizarlos por separado o mantenerlos en conjunto con el resto. De una u otra forma, al documentar nuestro proceder, es bueno dejar por escrito la razón por la que se tomaron ciertas desiciones y mostrar gráficamente el porqué para continuar con el análisis o estudio de la información.\n",
        "*Es Hora de comenzar a visualizar posibles respuestas a hipótesis o encausarlas.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxDZxJ7yZP3S"
      },
      "outputs": [],
      "source": [
        "#Estadísticas descriptivas\n",
        "df.describe()                 # media, std, min, percentiles\n",
        "df['edad'].value_counts()    # frecuencia de cada valor\n",
        "df.corr()                    # correlación entre variables numéricas\n",
        "\n",
        "#Histogramas y visualización\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df['edad'].hist(bins=10)\n",
        "sns.boxplot(x='genero', y='ingresos', data=df)\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "\n",
        "#Perfilado general (esto es más opcional, con librerías especiales)\n",
        "# Requiere instalar la librería primero: pandas-profiling\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "profile = ProfileReport(df)\n",
        "profile.to_notebook_iframe()  # muestra análisis automático"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Za3xcG7cu-R"
      },
      "source": [
        "# **Análisis exploratorio de datos (EDA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2HG-gIRdQlk"
      },
      "source": [
        " ### **Identificar Patrones, Tendencias y Outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i016B9Fjc462"
      },
      "source": [
        "* Analizar variables con groupby() y estadísticas agregadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualización de distribuciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in columnas_numericas:\n",
        "    plt.figure(figsize=(6,3))\n",
        "    sns.histplot(df[col], kde=True)\n",
        "    plt.title(f\"Distribución de {col}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Correlaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmU084r0de1h"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_scaled.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Matriz de correlaciones\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wje4flp5dIc-"
      },
      "source": [
        "* Detectar valores extremos/atípicos (outliers) con boxplot(), quantile(), z-scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J5CMCnndfWC"
      },
      "outputs": [],
      "source": [
        "#Este es solo un ejemplo de las miles de posibilidades de gráficos que se pueden usar:\n",
        "\n",
        "for col in columnas_numericas:\n",
        "    plt.figure(figsize=(6,3))\n",
        "    sns.boxplot(data=df_scaled[col])\n",
        "    plt.title(f\"Boxplot de {col}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conclusiones preliminares (EDA)**\n",
        "Por cada visualización que realicemos debemos realizar una observación, según dataset, de los resultados que vemos en la gráfica, dando paso al final, del por qué realizaremos algún ajuste para obtener un mejor resultado sobre lo observado. **Por ejemplo**:\n",
        " - Se detectaron outliers en las variables X, Y.\n",
        " - Hay alta correlación entre A y B.\n",
        " - Las variables categóricas fueron correctamente codificadas, por lo tanto esto quiere decir que... etc.\n",
        "\n",
        "**Consejos**: \n",
        "1. No escastimar en el uso de gráficos. Los gráficos nos permiten visualizar el panorama dando un mejor entendimiento de lo que estamos analizando, es mirar con otra perspectiva el panorama de los resultados. Quizás, nosotros entendemos dónde está el problema, qué se debe omitir porque no es del interés para resolución de hipótesis y sintamos que no son necesarios, pero recordá: **Lo que estás resolviendo no es solo para vos**, el resto de la humanidad, necesita entender tus presentaciones, por lo que un gráfico es un gran aliado.\n",
        "\n",
        "2. Estudiá y comprendé para qué sirve cada tipo de gráfico. No pongas uno porque se ve esteticamente genial y genera un impacto fabuloso. Cada gráfico tiene su propósito, reconocerlos te hará más eficaz en tu toma de decisiones para la resolución de hipótesis y hará de tu informe una lectura mas pertinente y entendible para aquellos que lo necesiten comprender pero no tienen tus conocimientos técnicos.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "No uses un mapa de calor si el dataframe que necesitas evaluar te muestras valores valores constantes y de repente algunos datos se disparan, es evidente que si tenés 300 datos que varían entre 20 y 50 y tu promedio o la media te dice 75, lo que necesitas es un gráfico de bigotes  o violín, buscar cuál o cuáles son esos valores atípicos y analizá si deberías sacarlos o no para tener más estabilidad en tu investigación ¿cómo saber si hacerlo o no? simple, **EL CONTEXTO de tu información**:\n",
        "\n",
        "Población en Argentina: si analizas Argentina incluyendo Capital Federal y por separado CABA, nunca vas a obtener el promedio de población, ni de trabajo, ni de natalidad o mortalidad del total en Argentina, ya que, en un espacio territorial mucho menor que Tucumán, vas a tener la misma o cercana población total de Formosa, Santiago del Estero, la media de Santa Fe o Córdoba (según periódo de información que se estudie). \n",
        "\n",
        "Entonces, no se puede excluir, pero se debe apartar estudiar las razones de ese salto máximo en los quartiles, conocer el contexto general para analizarlo, o, tomar la media entre CABA y Capital para que el número no sea tan exorbitante, siempre dejando en claro cuál fue tu decisión en cada paso.\n",
        "\n",
        "Y lo mismo te va a suceder con la mínima cuando se estudian Chubut y Tierra del fuego, por ejemplo Chubut tiene un Gran espacio territorial pero su población es mínima, las ciudades se encuentran a muchas horas de distancias por terrenos engorrosos entre ellos. Lo más seguro es que para estandarizar , busques excluir y trabajar por separado los mínimos y máximos.\n",
        "\n",
        "Ahora, Si estás haciendo estudios en un laboratorio de fórmula uno, en el que buscan la mayor potencia de un motor, en base a una mezcla de soluciones para aligerar la carga y, entre todos los resultados, ves que una o dos pruebas se disparan en la máxima ¡no las desestimes, es por ahí!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "C2HG-gIRdQlk",
        "ZAK63YtNdMkS"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
